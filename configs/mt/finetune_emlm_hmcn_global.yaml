experiment:
  dir: experiments
  name: mt_finetune_emlm_hmcn_global
  override: True
  seed: 42

train_dataset:
  name: trecis-multi-task-dataset
  params:
    data_path: materials/datasets/seed{}/train_masked.json
    encoder_high_lvl:
      key: high_info_type
      name: infotype2multilabel
      params:
        file: materials/meta/infotypes.json
        _type: high_info_type    
    encoder_low_lvl:
      key: low_info_type
      name: infotype2multilabel
      params:
        file: materials/meta/infotypes.json
        _type: low_info_type
    max_length: 128
    truncation: True
    pad_to_max_length: True
    cache_dir: 'hf'

eval_dataset:
  name: trecis-multi-task-dataset
  params:
    data_path: materials/datasets/seed{}/eval_masked.json
    encoder_high_lvl:
      key: high_info_type
      name: infotype2multilabel
      params:
        file: materials/meta/infotypes.json
        _type: high_info_type    
    encoder_low_lvl:
      key: low_info_type
      name: infotype2multilabel
      params:
        file: materials/meta/infotypes.json
        _type: low_info_type
    max_length: 128
    truncation: True
    pad_to_max_length: True
    cache_dir: 'hf'

test_dataset:
  name: trecis-multi-task-dataset
  params:
    data_path: materials/datasets/seed{}/test_masked.json
    encoder_high_lvl:
      key: high_info_type
      name: infotype2multilabel
      params:
        file: materials/meta/infotypes.json
        _type: high_info_type    
    encoder_low_lvl:
      key: low_info_type
      name: infotype2multilabel
      params:
        file: materials/meta/infotypes.json
        _type: low_info_type
    max_length: 128
    truncation: True
    pad_to_max_length: True
    cache_dir: 'hf'

tokenizer:
  name: bert-tokenizer
  params:
    model_name_or_path: bert-base-uncased
    add_special_tokens: [
      '[HASHTAG]',
      '[URL]',
      '[PERSON]',
      '[LOCATION]',
      '[ORGANIZATION]',
      '[EVENT]',
      '[ADDRESS]',
      '[PHONE_NUMBER]',
      '[DATE]',
      '[NUMBER]'
    ]
    use_fast: False
    cache_dir: 'hf'
    
model:
  name: hierarchical-global-task-head
  params:
    num_labels_high_lvl: 4
    num_labels_low_lvl: 25
    dropout: 0.1
    encoder:
      name: bert-encoder
      params:
        model_name_or_path: pretrained/seed{}/tapt_emlm/models
        cache_dir: 'hf'

loss:
  name: multi-task-loss
  weight_high_lvl: 0.1
  weight_low_lvl: 0.9
  weight_global: 1.0
  params: 
    losses: {'high_lvl': 'binary-cross-entropy', 'low_lvl': 'binary-cross-entropy', 'global': 'binary-cross-entropy'}

training:
  do_train: True
  do_eval: True
  do_predict: True
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  learning_rate: 0.00005
  weight_decay: 0.01
  warmup_ratio: 0.0
  num_train_epochs: 10
  evaluation_strategy: steps
  eval_steps: 1000
  save_strategy: steps
  save_steps: 1000
  save_total_limit: 10
  logging_strategy: steps
  logging_steps: 500
  load_best_model_at_end: True
  metric_for_best_model: f1_macro
  greater_is_better: True
  overwrite_output_dir: True